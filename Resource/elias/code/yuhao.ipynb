{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48500b49",
   "metadata": {},
   "source": [
    "## Yuhao Test  with same PDE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16321cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------ 数据加载与预处理 ------------------------\n",
    "\n",
    "def prepare_data(file_path, batch_size=32, test_size=0.2, group_key='mydata'):\n",
    "    df = pd.read_hdf(file_path, group_key)\n",
    "    n_samples, T = len(df), 180\n",
    "\n",
    "    X = np.zeros((n_samples, T, 2), dtype=np.float32)\n",
    "    for i, row in df.iterrows():\n",
    "        X[i, :, 0] = np.array(row['freq'], dtype=np.float32)\n",
    "        X[i, :, 1] = np.array(row['m_c'], dtype=np.float32)\n",
    "    Y = df[['w_t', 'l_t', 'Q', 'V']].values.astype(np.float32)\n",
    "\n",
    "    Xtr, Xte, Ytr, Yte = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "    sf = StandardScaler(); sm = StandardScaler(); st = StandardScaler()\n",
    "    flat_tr = Xtr.reshape(-1, 2); flat_te = Xte.reshape(-1, 2)\n",
    "    flat_tr[:, 0] = sf.fit_transform(flat_tr[:, 0:1]).ravel()\n",
    "    flat_tr[:, 1] = sm.fit_transform(flat_tr[:, 1:2]).ravel()\n",
    "    flat_te[:, 0] = sf.transform(flat_te[:, 0:1]).ravel()\n",
    "    flat_te[:, 1] = sm.transform(flat_te[:, 1:2]).ravel()\n",
    "    Xtr = flat_tr.reshape(-1, T, 2)\n",
    "    Xte = flat_te.reshape(-1, T, 2)\n",
    "    Ytr = st.fit_transform(Ytr)\n",
    "    Yte = st.transform(Yte)\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(Ytr)),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    te_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(Xte), torch.from_numpy(Yte)),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    return tr_loader, te_loader, sf, sm, st\n",
    "\n",
    "# ------------------------ 物理常数（部分可调整） ------------------------\n",
    "\n",
    "def get_constants(device):\n",
    "    return {\n",
    "        'E': torch.tensor(169e9, dtype=torch.float32, device=device),\n",
    "        'rho': torch.tensor(2330, dtype=torch.float32, device=device),\n",
    "        't': torch.tensor(25e-6, dtype=torch.float32, device=device),\n",
    "        'beta': torch.tensor(4.730041, dtype=torch.float32, device=device),\n",
    "        'm_coef_b': 0.39648,\n",
    "        'k_coef_b': 198.4629,\n",
    "        'k_coef_b3': 12.5643,\n",
    "        'electrode_length': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'electrode_width': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'w_c': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'l_c': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'd': torch.tensor(1e-6, dtype=torch.float32, device=device),\n",
    "        'Vac_ground': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'phi': torch.linspace(\n",
    "            math.pi/10,\n",
    "            math.pi/10 + math.pi/160,\n",
    "            180, device=device)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ 神经网络模型 ------------------------\n",
    "\n",
    "class PINN_MultiHead(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, T=180):\n",
    "        super().__init__()\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*T, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4), nn.ReLU()\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(hidden_dim//4, 1) for _ in range(4)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.trunk(x)\n",
    "        return torch.cat([h(z) for h in self.heads], dim=1)\n",
    "\n",
    "# ------------------------ 参数反归一化工具 ------------------------\n",
    "\n",
    "def calculate_params_vectorized(w_n, l_n, Q_n, V_n, consts, norm_sfs):\n",
    "    _, _, st = norm_sfs\n",
    "    out_mean = torch.tensor(st.mean_, device=w_n.device, dtype=torch.float32)\n",
    "    out_std  = torch.tensor(st.scale_, device=w_n.device, dtype=torch.float32)\n",
    "    w = w_n * out_std[0] + out_mean[0]\n",
    "    l = l_n * out_std[1] + out_mean[1]\n",
    "    Q = Q_n * out_std[2] + out_mean[2]\n",
    "    V = V_n * out_std[3] + out_mean[3]\n",
    "    eps = 1e-9\n",
    "\n",
    "    k_t  = (consts['k_coef_b'] / 12) * consts['E'] * consts['t'] * (w / (l + eps))**3\n",
    "    k_t3 =  consts['k_coef_b3'] * consts['E'] * consts['t'] * (w / (l + eps)**3)\n",
    "    Mass = consts['rho'] * (\n",
    "        consts['t'] * w * l * consts['m_coef_b']\n",
    "        + consts['electrode_length'] * consts['electrode_width'] * consts['t']\n",
    "        + 2 * consts['w_c'] * consts['l_c'] * consts['t']\n",
    "    )\n",
    "    trans = 8.85e-12 * V * consts['electrode_length'] * consts['t'] / (consts['d']**2)\n",
    "    k_e  = 2 * trans * V / consts['d']\n",
    "    k_e3 = 4 * trans * V / (consts['d']**3)\n",
    "    fac  = consts['Vac_ground'] * trans\n",
    "    c    = torch.sqrt(Mass * k_t) / Q\n",
    "\n",
    "    return {\n",
    "        'k_t':   k_t, 'k_t3':  k_t3, 'Mass':  Mass, 'k_e':   k_e, 'k_e3':  k_e3,\n",
    "        'fac':   fac, 'c':     c,    'trans': trans, 'phi':  consts['phi']\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aacd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ 物理PDE损失函数（核心！） ------------------------\n",
    "\n",
    "def spring_mass_damper_pde_loss(pred, Xb, constants, norm_sfs, denom_clamp=1e-4, loss_mode='clamp', clamp_val=1e-3):\n",
    "    \"\"\"\n",
    "    对应非线性弹簧-质量-阻尼动力学方程的物理loss，适用于PINN\n",
    "    \"\"\"\n",
    "    sf, sm, st = norm_sfs\n",
    "    freq  = Xb[:,:,0] * sf.scale_[0] + sf.mean_[0]\n",
    "    mc    = Xb[:,:,1] * sm.scale_[0] + sm.mean_[0]\n",
    "    omega = freq * (2 * math.pi)\n",
    "\n",
    "    w_n, l_n, Q_n, V_n = pred[:,0], pred[:,1], pred[:,2], pred[:,3]\n",
    "    p = calculate_params_vectorized(w_n, l_n, Q_n, V_n, constants, norm_sfs)\n",
    "\n",
    "    denom = (omega * p['trans'].unsqueeze(1)).clamp(min=denom_clamp)\n",
    "    A = (mc * 1e-9) / denom\n",
    "\n",
    "    kt, ke, kt3, ke3, mass, c, fac = [p[k].unsqueeze(1) for k in ['k_t','k_e','k_t3','k_e3','Mass','c','fac']]\n",
    "    phi = p['phi'].unsqueeze(0)\n",
    "\n",
    "    # 图中公式的sin/cos项残差\n",
    "    sin_res = -mass * A * omega**2 + (kt - ke) * A + (3/4) * (kt3 - ke3) * (A**3) - fac * torch.cos(phi)\n",
    "    cos_res = c * A * omega - fac * torch.sin(phi)\n",
    "\n",
    "    res_all = sin_res.pow(2) + cos_res.pow(2)\n",
    "    if loss_mode == 'clamp':\n",
    "        loss = res_all.clamp(max=clamp_val).mean()\n",
    "    else:\n",
    "        loss = res_all.mean()\n",
    "    return loss\n",
    "\n",
    "# ------------------------ 综合loss封装 ------------------------\n",
    "\n",
    "def pinn_total_loss(pred, Yb, Xb, constants, norm_sfs, \n",
    "                    lambda_data=1.0, lambda_phys=1.0,\n",
    "                    denom_clamp=1e-4, loss_mode='clamp', clamp_val=1e-3):\n",
    "    \"\"\"\n",
    "    总损失 = 数据MSE + 物理PDE残差loss\n",
    "    \"\"\"\n",
    "    L_data = nn.MSELoss()(pred, Yb)\n",
    "    L_phys = spring_mass_damper_pde_loss(pred, Xb, constants, norm_sfs, denom_clamp, loss_mode, clamp_val)\n",
    "    L_total = lambda_data * L_data + lambda_phys * L_phys\n",
    "    return L_total, L_data, L_phys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ 训练与验证循环 ------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, constants, norm_sfs, \n",
    "                   lambda_data, lambda_phys, denom_clamp, loss_mode, clamp_val, device):\n",
    "    model.train()\n",
    "    total_loss, total_data_loss, total_phys_loss = 0.0, 0.0, 0.0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(Xb)\n",
    "        L_total, L_data, L_phys = pinn_total_loss(\n",
    "            pred, Yb, Xb, constants, norm_sfs, lambda_data, lambda_phys, denom_clamp, loss_mode, clamp_val\n",
    "        )\n",
    "        L_total.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += L_total.item()\n",
    "        total_data_loss += L_data.item()\n",
    "        total_phys_loss += L_phys.item()\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_data_loss / n, total_phys_loss / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, loader, constants, norm_sfs,\n",
    "                      lambda_data, lambda_phys, denom_clamp, loss_mode, clamp_val, device):\n",
    "    model.eval()\n",
    "    total_loss, total_data_loss, total_phys_loss = 0.0, 0.0, 0.0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "        pred = model(Xb)\n",
    "        L_total, L_data, L_phys = pinn_total_loss(\n",
    "            pred, Yb, Xb, constants, norm_sfs, lambda_data, lambda_phys, denom_clamp, loss_mode, clamp_val\n",
    "        )\n",
    "        total_loss += L_total.item()\n",
    "        total_data_loss += L_data.item()\n",
    "        total_phys_loss += L_phys.item()\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_data_loss / n, total_phys_loss / n\n",
    "\n",
    "# ------------------------ 主训练流程 ------------------------\n",
    "\n",
    "def train_pinn(file_path, device, \n",
    "               lr=1e-3, weight_decay=1e-5, hidden_dim=256,\n",
    "               lambda_data=1.0, lambda_phys=1.0, \n",
    "               denom_clamp=1e-4, loss_mode='clamp', clamp_val=1e-3,\n",
    "               batch_size=32, epochs=500, patience=20):\n",
    "    tr_loader, te_loader, sf, sm, st = prepare_data(file_path, batch_size=batch_size)\n",
    "    norm_sfs = (sf, sm, st)\n",
    "    constants = get_constants(device)\n",
    "\n",
    "    model = PINN_MultiHead(hidden_dim=hidden_dim, T=180).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_data': [], 'val_data': [], 'train_phys': [], 'val_phys': []}\n",
    "    best_val = float('inf'); no_improv = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, tr_data, tr_phys = train_one_epoch(\n",
    "            model, tr_loader, optimizer, constants, norm_sfs, lambda_data, lambda_phys, denom_clamp, loss_mode, clamp_val, device)\n",
    "        val_loss, val_data, val_phys = validate_one_epoch(\n",
    "            model, te_loader, constants, norm_sfs, lambda_data, lambda_phys, denom_clamp, loss_mode, clamp_val, device)\n",
    "        scheduler.step(val_loss)\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_data'].append(tr_data)\n",
    "        history['val_data'].append(val_data)\n",
    "        history['train_phys'].append(tr_phys)\n",
    "        history['val_phys'].append(val_phys)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train: {tr_loss:.4g} (data {tr_data:.4g}, phys {tr_phys:.4g}) \"\n",
    "              f\"| Val: {val_loss:.4g} (data {val_data:.4g}, phys {val_phys:.4g})\")\n",
    "        if val_loss < best_val - 1e-5:\n",
    "            best_val = val_loss\n",
    "            no_improv = 0\n",
    "            torch.save(model.state_dict(), 'best_pinn.pth')\n",
    "        else:\n",
    "            no_improv += 1\n",
    "            if no_improv > patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35637bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ 可视化工具 ------------------------\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Train')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val')\n",
    "    plt.title('Total Loss'); plt.legend(); plt.grid()\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history['train_data'], label='Train Data')\n",
    "    plt.plot(epochs, history['val_data'], label='Val Data')\n",
    "    plt.title('Data Loss'); plt.legend(); plt.grid()\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history['train_phys'], label='Train Phys')\n",
    "    plt.plot(epochs, history['val_phys'], label='Val Phys')\n",
    "    plt.title('Physics Loss'); plt.legend(); plt.grid()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------------ 主程序入口 ------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b77a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 你的.h5文件路径\n",
    "    file_path = 'pinns_1.h5'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, history = train_pinn(\n",
    "        file_path=file_path,\n",
    "        device=device,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        hidden_dim=256,\n",
    "        lambda_data=1.0,\n",
    "        lambda_phys=1.0,\n",
    "        denom_clamp=1e-4,\n",
    "        loss_mode='clamp',\n",
    "        clamp_val=1e-3,\n",
    "        batch_size=32,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "    )\n",
    "    plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e4b8e",
   "metadata": {},
   "source": [
    "## 更改 v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25d7c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train: 0.9991 (data 0.006438, phys 0.9926) | Val: 1 (data 0, phys 1)\n",
      "Epoch 2/200 | Train: 1 (data 0, phys 1) | Val: 1 (data 0, phys 1)\n",
      "Epoch 3/200 | Train: 1 (data 0, phys 1) | Val: 1 (data 0, phys 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 280\u001b[0m\n\u001b[1;32m    278\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinns_1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    279\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pinn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenom_clamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m plot_training_history(history)\n",
      "Cell \u001b[0;32mIn[37], line 232\u001b[0m, in \u001b[0;36mtrain_pinn\u001b[0;34m(file_path, device, lr, weight_decay, hidden_dim, penalty, denom_clamp, batch_size, epochs, patience)\u001b[0m\n\u001b[1;32m    229\u001b[0m best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m); no_improv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 232\u001b[0m     tr_loss, tr_data, tr_phys \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom_clamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     val_loss, val_data, val_phys \u001b[38;5;241m=\u001b[39m validate_one_epoch(\n\u001b[1;32m    235\u001b[0m         model, te_loader, norm_dict, consts, penalty, denom_clamp, device)\n\u001b[1;32m    236\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_loss)\n",
      "Cell \u001b[0;32mIn[37], line 192\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, norm_dict, consts, penalty, denom_clamp, device)\u001b[0m\n\u001b[1;32m    190\u001b[0m L_total\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    191\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 192\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L_total\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    194\u001b[0m total_data_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m L_data\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/kul/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/kul/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/kul/lib/python3.10/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/miniconda3/envs/kul/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/kul/lib/python3.10/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/kul/lib/python3.10/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------ 数据加载与预处理（nan保留） ------------------------\n",
    "\n",
    "def prepare_data(file_path, batch_size=32, test_size=0.2, group_key='mydata'):\n",
    "    df = pd.read_hdf(file_path, group_key)\n",
    "    df = df[df[['w_t','l_t','Q','V']].notnull().all(axis=1)].reset_index(drop=True)\n",
    "    n_samples, T = len(df), 180\n",
    "\n",
    "    X = np.zeros((n_samples, T, 2), dtype=np.float32)\n",
    "    for i, row in df.iterrows():\n",
    "        X[i, :, 0] = np.array(row['freq'], dtype=np.float32)  # 这里允许nan\n",
    "        X[i, :, 1] = np.array(row['m_c'], dtype=np.float32)\n",
    "    Y = df[['w_t', 'l_t', 'Q', 'V']].values.astype(np.float32)\n",
    "\n",
    "    Xtr, Xte, Ytr, Yte = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # 标准化freq和m_c时跳过nan\n",
    "    def nan_scale_fit(x): return (np.nanmean(x), np.nanstd(x) + 1e-12)\n",
    "    freq_mean, freq_std = nan_scale_fit(Xtr[..., 0])\n",
    "    mc_mean, mc_std     = nan_scale_fit(Xtr[..., 1])\n",
    "    for arr in [Xtr, Xte]:\n",
    "        arr[..., 0] = (arr[..., 0] - freq_mean) / freq_std\n",
    "        arr[..., 1] = (arr[..., 1] - mc_mean)   / mc_std\n",
    "\n",
    "    # 目标归一化不涉及nan\n",
    "    y_mean, y_std = np.mean(Ytr, axis=0), np.std(Ytr, axis=0) + 1e-12\n",
    "    Ytr = (Ytr - y_mean) / y_std\n",
    "    Yte = (Yte - y_mean) / y_std\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(Xtr), torch.from_numpy(Ytr)),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    te_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(Xte), torch.from_numpy(Yte)),\n",
    "        batch_size=batch_size)\n",
    "    # 用标量均值/方差对象返回，方便后续反归一化\n",
    "    norm_dict = {\n",
    "        'freq': (freq_mean, freq_std),\n",
    "        'mc':   (mc_mean, mc_std),\n",
    "        'y':    (y_mean, y_std)\n",
    "    }\n",
    "    return tr_loader, te_loader, norm_dict\n",
    "\n",
    "# ------------------------ 物理常数 ------------------------\n",
    "\n",
    "def get_constants(device):\n",
    "    return {\n",
    "        'E': torch.tensor(169e9, dtype=torch.float32, device=device),\n",
    "        'rho': torch.tensor(2330, dtype=torch.float32, device=device),\n",
    "        't': torch.tensor(25e-6, dtype=torch.float32, device=device),\n",
    "        'beta': torch.tensor(4.730041, dtype=torch.float32, device=device),\n",
    "        'm_coef_b': 0.39648,\n",
    "        'k_coef_b': 198.4629,\n",
    "        'k_coef_b3': 12.5643,\n",
    "        'electrode_length': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'electrode_width': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'w_c': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'l_c': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'd': torch.tensor(1e-6, dtype=torch.float32, device=device),\n",
    "        'Vac_ground': torch.tensor(1e-3, dtype=torch.float32, device=device),\n",
    "        'phi': torch.linspace(\n",
    "            math.pi/10,\n",
    "            math.pi/10 + math.pi/160,\n",
    "            180, device=device)\n",
    "    }\n",
    "\n",
    "# ------------------------ 网络结构 ------------------------\n",
    "\n",
    "class PINN_MultiHead(nn.Module):\n",
    "    def __init__(self, hidden_dim=256, T=180):\n",
    "        super().__init__()\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*T, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4), nn.ReLU()\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(hidden_dim//4, 1) for _ in range(4)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.trunk(x)\n",
    "        return torch.cat([h(z) for h in self.heads], dim=1)\n",
    "\n",
    "# ------------------------ 参数反归一化 ------------------------\n",
    "\n",
    "def denorm_pred(pred, norm_dict):\n",
    "    y_mean = torch.tensor(norm_dict['y'][0], device=pred.device, dtype=torch.float32)\n",
    "    y_std  = torch.tensor(norm_dict['y'][1], device=pred.device, dtype=torch.float32)\n",
    "    return pred * y_std + y_mean\n",
    "\n",
    "# ------------------------ nan mask数据loss ------------------------\n",
    "\n",
    "def robust_mse(pred, target):\n",
    "    mask = torch.isfinite(pred) & torch.isfinite(target)\n",
    "    n_valid = mask.sum()\n",
    "    if n_valid == 0:\n",
    "        return torch.tensor(0., device=pred.device)\n",
    "    diff = (pred - target)[mask]\n",
    "    return (diff ** 2).mean()\n",
    "\n",
    "# ------------------------ nan mask物理loss ------------------------\n",
    "\n",
    "def physics_loss_with_nan_penalty(freq_res, disp_res, penalty=1.0):\n",
    "    valid = torch.isfinite(freq_res) & torch.isfinite(disp_res)\n",
    "    f = torch.where(valid, freq_res, torch.zeros_like(freq_res))\n",
    "    d = torch.where(valid, disp_res, torch.zeros_like(disp_res))\n",
    "    sq = f.pow(2) + d.pow(2)\n",
    "    sum_sq = sq.sum(dim=1)\n",
    "    n_valid = valid.sum(dim=1).float()\n",
    "    T = freq_res.size(1)\n",
    "    n_invalid = T - n_valid\n",
    "    avg_valid = sum_sq / torch.clamp(n_valid, min=1.0)\n",
    "    frac_invalid = n_invalid / T\n",
    "    loss_per_sample = avg_valid + penalty * frac_invalid\n",
    "    return loss_per_sample.mean()\n",
    "\n",
    "# ------------------------ 物理PDE残差 ------------------------\n",
    "\n",
    "def spring_mass_damper_pde_residuals(pred, Xb, norm_dict, consts, denom_clamp=1e-4):\n",
    "    # 反标准化freq/mc\n",
    "    freq_mean, freq_std = norm_dict['freq']\n",
    "    mc_mean, mc_std = norm_dict['mc']\n",
    "    freq = Xb[:,:,0] * freq_std + freq_mean\n",
    "    mc   = Xb[:,:,1] * mc_std   + mc_mean\n",
    "    omega = freq * (2 * math.pi)\n",
    "\n",
    "    # 反归一化 pred\n",
    "    w_n, l_n, Q_n, V_n = [pred[:,i] for i in range(4)]\n",
    "    y_mean, y_std = norm_dict['y']\n",
    "    w = w_n * y_std[0] + y_mean[0]\n",
    "    l = l_n * y_std[1] + y_mean[1]\n",
    "    Q = Q_n * y_std[2] + y_mean[2]\n",
    "    V = V_n * y_std[3] + y_mean[3]\n",
    "\n",
    "    eps = 1e-9\n",
    "    k_t  = (consts['k_coef_b'] / 12) * consts['E'] * consts['t'] * (w / (l + eps))**3\n",
    "    k_t3 =  consts['k_coef_b3'] * consts['E'] * consts['t'] * (w / (l + eps)**3)\n",
    "    Mass = consts['rho'] * (\n",
    "        consts['t'] * w * l * consts['m_coef_b']\n",
    "        + consts['electrode_length'] * consts['electrode_width'] * consts['t']\n",
    "        + 2 * consts['w_c'] * consts['l_c'] * consts['t']\n",
    "    )\n",
    "    trans = 8.85e-12 * V * consts['electrode_length'] * consts['t'] / (consts['d']**2)\n",
    "    k_e  = 2 * trans * V / consts['d']\n",
    "    k_e3 = 4 * trans * V / (consts['d']**3)\n",
    "    fac  = consts['Vac_ground'] * trans\n",
    "    c    = torch.sqrt(Mass * k_t) / Q\n",
    "\n",
    "    denom = (omega * trans.unsqueeze(1)).clamp(min=denom_clamp)\n",
    "    A = (mc * 1e-9) / denom\n",
    "\n",
    "    # 扩展物理参数形状\n",
    "    kt, ke, kt3, ke3, mass, c_, fac_ = [x.unsqueeze(1) for x in [k_t,k_e,k_t3,k_e3,Mass,c,fac]]\n",
    "    phi = consts['phi'].unsqueeze(0)\n",
    "\n",
    "    sin_res = -mass * A * omega**2 + (kt - ke) * A + (3/4) * (kt3 - ke3) * (A**3) - fac_ * torch.cos(phi)\n",
    "    cos_res = c_ * A * omega - fac_ * torch.sin(phi)\n",
    "    return cos_res, sin_res\n",
    "\n",
    "# ------------------------ 综合loss ------------------------\n",
    "\n",
    "def pinn_total_loss(pred, Yb, Xb, norm_dict, consts, penalty=1.0, denom_clamp=1e-4):\n",
    "    L_data = robust_mse(pred, Yb)\n",
    "    freq_res, disp_res = spring_mass_damper_pde_residuals(pred, Xb, norm_dict, consts, denom_clamp)\n",
    "    L_phys = physics_loss_with_nan_penalty(freq_res, disp_res, penalty=penalty)\n",
    "    L_total = L_data + L_phys\n",
    "    return L_total, L_data, L_phys\n",
    "\n",
    "# ------------------------ 训练与验证循环 ------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, norm_dict, consts, penalty, denom_clamp, device):\n",
    "    model.train()\n",
    "    total_loss, total_data_loss, total_phys_loss = 0.0, 0.0, 0.0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(Xb)\n",
    "        L_total, L_data, L_phys = pinn_total_loss(\n",
    "            pred, Yb, Xb, norm_dict, consts, penalty, denom_clamp\n",
    "        )\n",
    "        L_total.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += L_total.item()\n",
    "        total_data_loss += L_data.item()\n",
    "        total_phys_loss += L_phys.item()\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_data_loss / n, total_phys_loss / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, loader, norm_dict, consts, penalty, denom_clamp, device):\n",
    "    model.eval()\n",
    "    total_loss, total_data_loss, total_phys_loss = 0.0, 0.0, 0.0\n",
    "    for Xb, Yb in loader:\n",
    "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "        pred = model(Xb)\n",
    "        L_total, L_data, L_phys = pinn_total_loss(\n",
    "            pred, Yb, Xb, norm_dict, consts, penalty, denom_clamp\n",
    "        )\n",
    "        total_loss += L_total.item()\n",
    "        total_data_loss += L_data.item()\n",
    "        total_phys_loss += L_phys.item()\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_data_loss / n, total_phys_loss / n\n",
    "\n",
    "# ------------------------ 主训练流程 ------------------------\n",
    "\n",
    "def train_pinn(file_path, device, \n",
    "               lr=1e-3, weight_decay=1e-5, hidden_dim=256,\n",
    "               penalty=1.0, denom_clamp=1e-4,\n",
    "               batch_size=32, epochs=500, patience=20):\n",
    "    tr_loader, te_loader, norm_dict = prepare_data(file_path, batch_size=batch_size)\n",
    "    consts = get_constants(device)\n",
    "\n",
    "    model = PINN_MultiHead(hidden_dim=hidden_dim, T=180).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_data': [], 'val_data': [], 'train_phys': [], 'val_phys': []}\n",
    "    best_val = float('inf'); no_improv = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss, tr_data, tr_phys = train_one_epoch(\n",
    "            model, tr_loader, optimizer, norm_dict, consts, penalty, denom_clamp, device)\n",
    "        val_loss, val_data, val_phys = validate_one_epoch(\n",
    "            model, te_loader, norm_dict, consts, penalty, denom_clamp, device)\n",
    "        scheduler.step(val_loss)\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_data'].append(tr_data)\n",
    "        history['val_data'].append(val_data)\n",
    "        history['train_phys'].append(tr_phys)\n",
    "        history['val_phys'].append(val_phys)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train: {tr_loss:.4g} (data {tr_data:.4g}, phys {tr_phys:.4g}) \"\n",
    "              f\"| Val: {val_loss:.4g} (data {val_data:.4g}, phys {val_phys:.4g})\")\n",
    "        if val_loss < best_val - 1e-5:\n",
    "            best_val = val_loss\n",
    "            no_improv = 0\n",
    "            torch.save(model.state_dict(), 'best_pinn.pth')\n",
    "        else:\n",
    "            no_improv += 1\n",
    "            if no_improv > patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "    return model, history\n",
    "\n",
    "# ------------------------ 可视化 ------------------------\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Train')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val')\n",
    "    plt.title('Total Loss'); plt.legend(); plt.grid()\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history['train_data'], label='Train Data')\n",
    "    plt.plot(epochs, history['val_data'], label='Val Data')\n",
    "    plt.title('Data Loss'); plt.legend(); plt.grid()\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history['train_phys'], label='Train Phys')\n",
    "    plt.plot(epochs, history['val_phys'], label='Val Phys')\n",
    "    plt.title('Physics Loss'); plt.legend(); plt.grid()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------------ 主程序入口 ------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'pinns_1.h5'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, history = train_pinn(\n",
    "        file_path=file_path,\n",
    "        device=device,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        hidden_dim=256,\n",
    "        penalty=1.0,\n",
    "        denom_clamp=1e-4,\n",
    "        batch_size=32,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "    )\n",
    "    plot_training_history(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
